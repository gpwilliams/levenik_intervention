---
title: "Results"
author: "Glenn Williams"
date: "14<sup>th</sup> August, 2019 (Last updated: `r format(Sys.time(), '%d<sup>th</sup> %B, %Y')`)"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: united
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)

packages <- c(
  "tidyverse",
  "rlang",
  "here",
  "brms",
  "tidybayes",
  "bayestestR",
  "modelr",
  "ggforce",
  "ggrepel",
  "ggridges",
  "irr",
  "kableExtra"
)

# load packages
lapply(packages, library, character.only = TRUE)

# load functions
r_function_list <- list.files(
  path = here("R", "00_functions"), 
  pattern = "R$",
  full.names = TRUE
)
purrr::walk(r_function_list, source)

# get citations
citations <- purrr::map(packages, citation)

# get file paths ----

# get model summaries only for easy loading
fitted_models_dir <- list.files(
  path = here("04_analysis", "01_models"),
  pattern = "rds$",
  full.names = TRUE
) %>% 
  str_subset("summary")
  

model_summaries_dir <- list.files(
  path = here("04_analysis", "02_summaries", "02_full-data"),
  pattern = "csv$",
  full.names = TRUE
)

plots_dir <- list.files(
  path = here("03_plots", "02_full-data"),
  pattern = "png$",
  full.names = TRUE
)

# load data ----

fitted_models <- purrr::map(fitted_models_dir, read_rds)

names(fitted_models) <- fitted_models_dir %>% 
  sub(".*/", "", .) %>% 
  substr(., 1, nchar(.)-4)

model_summaries <- purrr::map(model_summaries_dir, read_csv)
names(model_summaries) <- model_summaries_dir %>% 
  sub(".*/", "", .) %>% 
  substr(., 1, nchar(.)-4)
```

```{r define-renaming-variables}
summary_oldnames <- c(".value", "interval")
summary_newnames <- c("median", "percentile interval")
summary_drop <- c(".point", ".interval")

posterior_oldnames <- c("Ci Interval", "Rope Interval", "Pd")
posterior_newnames <- c("Percentile Interval", "HDI Interval", "P(Direction)")
posterior_drop <- c(
  ".width",
  ".point",
  ".interval",
  "CI",
  "ROPE_low", 
  "ROPE_high",
  "ROPE_Equivalence"
)

model_replacements = c(
  "word_type1" = "word_type",
  "word_type2" = "word_familiarity",
  "language_variety1" = "language_variety_1", # figure out what these are
  "language_variety2" = "language_variety_2",
  "language_variety3" = "language_variety_3",
  "task1" = "task",
  "mean_exposure_test_nLED" = "mean_vocabulary_test_nLED"
)
```

`r version$version.string` and the R-packages `r cite_packages(packages)` were used for data preparation, analysis, and presentation.

# Models

The data were analysed using Bayesian distributional models in the brms R-package. These models account for the fact that nLEDs are bounded between 0 and 1, with inflated counts at these bounds on a trial-by-trial basis, which results in non-normal distributions. Crucially, in contrast to general linear models and linear mixed effects models these, models do not make predictions outside the possible range of values and accurately capture the larger densities at extreme values. At the time of writing, distributional models of this nature are only available for hierarchical data using the brms R-package, which requires model fitting to be performed using a Bayesian framework. As an additional benefit, Baeysian models do not suffer from the non-convergence associated with modelling complex analyses under a Frequentist framework.

## Zero-one Inflated Beta Distributions

The models were fitted using a zero-one inflated Beta distribution, which models the data as a Beta distribution for nLEDs excluding 0 and 1, and a Bernoulli distribution for binary nLEDs of 0 and 1. Thus, predictors in the model can affect four distributional parameters: $\mu$ (mu), the mean of the nLEDs excluding 0 and 1; $\phi$ (phi), the precision (i.e. spread) of the nLEDs excluding 0 and 1; $\alpha$ (alpha; termed zoi - or zero-one inflation in brms) the probability of an nLED of 0 or 1; and $\gamma$ (gamma; termed coi - or conditional-one inflation in brms), the conditional probability of a 1 given a 0 or 1 has been observed. Larger values for these parameters are associated with (a) higher mean nLEDs in the range exluding 0 and 1, (b) tighter distributions of the nLEDs in the range excluding 0 and 1 (i.e. less variance), (c) more zero-one inflation in nLEDs, and (d) more one-inflation given zero-inflation in nLEDs. Predictors in this model can influence any and all distributional parameters in the model at once. For these models, a logit link is used for the $\mu$, $\alpha$, and $\gamma$ distributional parameters, and a log link is used for the $\phi$ distributional parameter.

## Model Fitting

### Model Specification

Three models were fitted in total: (1) assessing performance across conditions during the vocabulary test prior to literacy training; (2) assessing performance across conditions during the testing phase following literacy training; and (3) assessing performance across conditions during the testing phase following literacy training using the vocabulary test performance as a predictor. This latter model was not pre-registered, but instead serves an exploratory purpose to determine whether or not any effect of dialect exposure is mediated by initial performance. In all models, estimates population-level and group-level effects are estimated for all distributional parameters, with group-level effects correlated across all parameters.

The models were described as follows:

- **Vocabulary Test Model**: nLEDs are predicted by population-level (fixed) effects of Variety Exposure condition (with four levels: Variety Match, Mismatch, Mismatch Social, and Dialect Learning) Word Type (with two levels: Contrative and Non-contrastive), and the interaction between them, and by group-level (random) effects of random intercepts and slopes of Word Type by participants, and random intercepts and slopes of Variety Exposure by item.

- **Testing Model**: nLEDs are predicted by population-level (fixed) effects of Task (with two levels: Reading and Spelling), Variety Exposure condition, and Word Type and the interaction between them, and by group-level (random) effects of random intercepts and slopes of Task and Word Type by participant, and random intercepts and slopes of Variety Exposure by item. Crucially, the interaction between the group-level effects by participant did not include the interaction between them in order to reduce model complexity.

- **Exploratory Covariate Testing Model**: nLEDs are predicted by population-level (fixed) effects of mean nLED during the Vocabulary Test, Task, Variety Exposure condition, Word Type, and the interaction between them, and by group-level (random) effects of random intercepts and slopes of Task and Word Type by participant, and random intercepts and slopes of mean nLED during the Vocabulary Test and Variety Exposure by items. Again, the interaction between the group-level effects by participant did not include the interaction between them in order to reduce model complexity.

### Model Priors

In all models, the approach was to use weakly informative, regularising priors for fitting. Where models failed to converge, these priors were adjusted, typically placing less prior weight on extreme values.

Here, priors are described first by their expected distribution, and the parameters that define that distribution. For example, a prior of $\mathcal{N}(0, 1)$ describes a normal distribution with a mean of 0 and a standard deviation of 1. Similarly, a prior of $\mathcal{logistic}(0, 1)$ describes a logistic distribution with a mean of 0 and a standard deviation of 1. Note, by default, brms restricts priors on the *SD* to be positive.

The following priors were used for the exposure model:

- Intercept
  - $\mu$: $\mathcal{N}(0, 5)$
  - $\phi$: $\mathcal{N}(0, 3)$
  - $\alpha$: $\mathcal{logistic}(0, 1)$
  - $\gamma$: $\mathcal{logistic}(0, 1)$
- Slope
  - $\mu$: $\mathcal{N}(0, 0.5)$
  - $\phi$: $\mathcal{N}(0, 1)$
  - $\alpha$: $\mathcal{N}(0, 5)$
  - $\gamma$: $\mathcal{N}(0, 0.5)$
- *SD*
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 1)$
  - $\alpha$: $\mathcal{N}(0, 5)$
  - $\gamma$: $\mathcal{N}(0, 5)$
- *SD* by Participant Number
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 5)$
  - $\alpha$: $\mathcal{N}(0, 10)$
  - $\gamma$: $\mathcal{N}(0, 10)$
- *SD* by Item
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 5)$
  - $\alpha$: $\mathcal{N}(0, 10)$
  - $\gamma$: $\mathcal{N}(0, 10)$
- Correlation
  - $LKJ(2)$
  
Weakly informative regularising priors were used for all terms. All priors were centred on 0, with standard deviations ranging from 0.5 to 10, thus allowing for a range of values with less prior probability places on extreme responses. Largely, these priors allow the posterior to be determined primarily by the data. For the slope terms, the priors assume no effect to small effects for each parameter in either direction. Weakly informative regularising priors were also used for all standard deviation terms. Finally, an $LKJ(2)$ prior was used for the correlation between terms, which acts to down-weight perfect correlations (Vasishth et al., 2018 - CITATION). These priors are in some cases more informative than initially planned following our pre-registration (which used very weakly informative priors) to improve model fit (i.e. accounting for divergences during fitting). For example, the mu intercept and slope, and gamma slope have standard deviations half as large as planned, while the standard deviation for the phi intercept is three times as large as initially planned. Additionally, 8000 iterations were used instead of 1000 and 6 were used rather than 4 chains to improve estimates in response to warnings about bulk and tail effective sample size, totalling 48,000 samples rather than the planned 4000.

For both testing models, the following priors were used:

- Intercept
  - $\mu$: $\mathcal{N}(0, 5)$
  - $\phi$: $\mathcal{N}(0, 3)$
  - $\alpha$: $\mathcal{logistic}(0, 1)$
  - $\gamma$: $\mathcal{logistic}(0, 1)$
- Slope
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 1)$
  - $\alpha$: $\mathcal{N}(0, 5)$
  - $\gamma$: $\mathcal{N}(0, 1)$
- *SD*
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 1)$
  - $\alpha$: $\mathcal{N}(0, 5)$
  - $\gamma$: $\mathcal{N}(0, 5)$
- *SD* by Participant Number
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 5)$
  - $\alpha$: $\mathcal{N}(0, 10)$
  - $\gamma$: $\mathcal{N}(0, 10)$
- *SD* by Item
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 5)$
  - $\alpha$: $\mathcal{N}(0, 10)$
  - $\gamma$: $\mathcal{N}(0, 10)$
- Correlation
  - $LKJ(2)$
  
Due to having more observations for analyses during the testing phase, both the $\mu$ and $\gamma$ slope terms use more weakly informative priors than the exposure model. This allows the data to have a larger impact on parameter estimates while having no impact on model convergence. 

### Model Checks

Posterior predictive checks were performed for all three models, comparing the observed posterior density against samples from the fitted model. Well fitting models show concordance between observed and sampled posterior densities. Plots for each model are displayed below. Grey lines indicate samples from the posterior, while black lines indicate the observed sample density.

```{r pp-check-plots}
knitr::include_graphics(plots_dir[str_detect(plots_dir, "pp")])
```

As can be seen from the plots, the posterior predictive checks indicate a generally good model fit in all instances, such that the model largely captures the shape of the data (i.e. especially capturing the 0 and 1 inflation in the testing model), but does not capture some discrepancies in the data which do not arise from any particular process (i.e. some larger densities in the testing model between the range of 0-1).

# Vocabulary Test Model

A summary of the population-level (fixed) effects for the Vocabulary Test model is provided below. This can be used to determine model diagnostics, coefficients, and estimates around these coefficients using 95% credible intervals. To answer questions pertaining to our pre-registered hypotheses, and to generate plots for these summaries, we used draws from the posterior for different combinations of conditions using the `r cite_package("tidybayes")` R-package. 

In all following plots and reported statistics, summaries are provided for for the joint posterior of the model taking into all distributional parameters during sampling. This provides an overall nLED for any comparison, rather than separate estimates of nLEDs between the bounds of 0 and 1 and for the extremes of 0 and 1. For reported results in tables, estimates are based on the median and credible interval around the median. The median was selected to summarise these models over the mean as this method is more robust to distributions with more than one mode. Thus, we do not provide individual statistics and plots for the individual distributional terms (e.g. for zero-one inflation, or conditional-one inflation) as we did not specify any hypotheses related to these individual terms. Instead, the zero-one inflated Beta models are used purely to improve model fit and to make more accurate predictions about the overall differences in nLEDs across conditions. Here, 90% credible intervals are used to summarise uncertainty in the estimates are more stable than wider intervals when given a limited number of draws from the posterior (Kruschke, 2014).

To determine support for hypotheses using these estimates, the probability of direction $P(direction)$, or *pd*, is provided. This is defined as the proportion of the posterior that is of the same sign as the median. In previous simulations, the *pd* has been found to be linearly related to the frequentist *p*-value (Makowski et al., 2019). The *pd* therefore provides an index of the existence of an effect outlining certainty in whether an effect is positive or negative. This can be used to ultimately reject the null hypothesis, but like the frequentist *p*-value does not give a reliable estimate of evidence in support of the null hypothesis. Additional hypothesis tests are provided in the form of Region of Practical Equivalence (ROPE) analyses from these draws using the `r cite_package("bayestestR")` R-package. This defines an area around the point null that is practically equivalent to zero for assessing evidence in support of the null hypothesis (Krushke, 2014). Here, the bounds of the ROPE range are defined as half the smallest effect reported in the Williams et al. (forthcoming) parameter estimates and intervals report the 90% highest density interval (HDI) of the posterior. The HDI differs from the equal tailed intervals used for summary statistics in that values within the range are always more probable than values outside of the range, and the interval need not exclude an equal amount of the distribution towards both tails. With symmetric distributions, the two methods produce similar results.

In plots, posterior medians and 80% and 90% credible intervals are provided for different conditions in the plots below. Table summaries also provide posterior medians along with 90% credible intervals.

In the tables of population level (fixed) effects, $\hat{R}$ is a measure of convergence for within- and between-chain estimates, with values closer to 1 being preferable. The bulk and tail effective sample sizes give diagnostics of the number of draws which contain the same amount of information as the dependent sample (CITE STAN WEBSITE), with higher values being preferable. The tail effective sample size is determined at the 5% and 95% quantiles, while the bulk is determined at values in between these quantiles.

```{r exposure-model-summary, results = "asis"}
fitted_models[["exposure_model_summary"]]$fixed %>%
  as.data.frame() %>% 
  rownames_to_column(., var = "Parameter") %>% 
  merge_CI_limits(., `l-95% CI`, `u-95% CI`, decimals = 3) %>%
  select(Parameter:Est.Error, interval, everything()) %>% 
  split_dists() %>% 
  mutate(distribution = factor(distribution, levels = c("mu", "phi", "zoi", "coi"))) %>% 
  arrange(distribution) %>%
  rename_parameters(replacements = model_replacements) %>%
  table_cols_to_title(.) %>% 
  select(-Distribution) %>% 
  custom_kable() %>% 
  pack_rows(index = c("$\\mu$" = 8, "$\\phi$" = 8, "$\\alpha$" = 8, "$\\gamma$" = 8))
```

## Variety Exposure

```{r variety-exposure-condition-plot-exposure, fig.show = "hold", out.width = "50%"}
knitr::include_graphics(c(
  plots_dir[str_detect(plots_dir, "exposure_v.png")],
  plots_dir[str_detect(plots_dir, "exposure_v_compare.png")]
))
```

Posterior means and 90% credible intervals are provided in the table below.

```{r variety-exposure-condition-exposure}
format_summary_table(
  model_summaries$exposure_v, 
  summary_oldnames, 
  summary_newnames,
  .drop = summary_drop
  ) %>% 
  filter(.Width == 0.90) %>% 
  select(-.Width) %>% 
  custom_kable()
```

The differences between conditions were compared using the `compare_levels()` function from the `r cite_package("tidybayes")` R-package. This allows for a direct comparison of differences between groups, which provides a more accurate and reliable method of establishing group differences than visual inspection of whether credible intervals overlap from estimates of the individual groups (Schenker & Gentleman, 2001). Here, the posterior is summarised as the median and 90% credible interval around the median. We also evaluated equivalence in the nLEDs between variety exposure conditions by determining a region of practical equivalence (ROPE) by which any effects between the reported bounds are deemed to be practically equivalent to 0. In all instances, this is determined at the 90% credible interval (CI) bound of the highest density interval (HDI). We report the proportion of the HDI contained within the ROPE region along with bounds of this interval. Where HDIs are entirely contained by the equivalence bounds, equivalence is accepted. Where HDIs are entirely outside the equivalence bounds, equivalence is rejected. Uncertainty is assigned to any HDIs that cross the equivalence bounds in either (or both) directions. Finally, the probability of direction is reported, showing the proportion of the posterior that is of the median's sign (i.e. how much of the posterior supports a positive or negative effect).

```{r variety-exposure-condition-exposure-compare}
report_posteriors(
  model_summaries$exposure_v_compare,
  model_summaries$exposure_v_compare_rope,
  0.9,
  posterior_drop,
  posterior_oldnames,
  posterior_newnames
) %>% 
  custom_kable(., add_footer = TRUE, model_summaries$exposure_v_compare_rope)
```

While nLEDs in the Variety Mismatch condition are generally lower than those in the Variety Match condition, 30% of the difference scores are contained by the equivalence bounds, and approximately 91% of the difference is of the same sign as the median.

Similarly, while nLEDs are generally higher in the two intervention conditions (Variety Mismatch Social and Dialect Literacy) when compared to the Variety Mismatch condition, around half of the difference scores are contained by the equivalence bounds. All other differences are largely undecided.

## Word Type by Variety Exposure

We also looked at whether there are any differences in performance for different word types across conditions during the vocabulary testing phase. 

```{r variety-word-type-exposure-plot, fig.show = "hold", out.width = "50%"}
knitr::include_graphics(c(
  plots_dir[str_detect(plots_dir, "exposure_vw.png")],
  plots_dir[str_detect(plots_dir, "exposure_vw_compare.png")]
))
```

Posterior means and credible intervals are provided in the table below.

```{r variety-word-type-exposure}
format_summary_table(
  model_summaries$exposure_vw, 
  summary_oldnames, 
  summary_newnames,
  .drop = summary_drop
  ) %>% 
  filter(.Width == 0.90) %>% 
  select(-.Width) %>% 
  custom_kable()
```

Comparisons between levels of word type during the exposure phase are shown for each variety exposure condition below. Here, all effects span either side of 0, suggesting that there is no reliable difference between word type by variety exposure condition in the exposure phase. In all instances there is some evidence that performance is better for non-contrastive words relative to contrastive words. However, the HDI around the parameter spans zero in all instances, and only a small proportion of the HDI is contained by the ROPE range.

```{r variety-word-type-exposure-compare}
report_posteriors(
  model_summaries$exposure_vw_compare,
  model_summaries$exposure_vw_compare_rope,
  0.9,
  posterior_drop,
  posterior_oldnames,
  posterior_newnames
) %>% 
  custom_kable(., add_footer = TRUE, model_summaries$exposure_vw_compare_rope)
```

# Testing Phase Model

A summary of the Testing Phase model is provided below. This can be used to determine model diagnostics and coefficients. As with the Vocabulary Test Model, to answer questions pertaining to our pre-registered hypotheses, and to generate plots for these summaries, we used draws from the posterior for different combinations of conditions. Similarly, hypothesis tests are provided in the form of ROPE analyses and *pd*. 

```{r testing-model-summary}
fitted_models[["testing_model_summary"]]$fixed %>% 
  as.data.frame() %>% 
  rownames_to_column(., var = "Parameter") %>% 
  merge_CI_limits(., `l-95% CI`, `u-95% CI`, decimals = 3) %>%
  select(Parameter:Est.Error, interval, everything()) %>% 
  split_dists() %>% 
  mutate(distribution = factor(distribution, levels = c("mu", "phi", "zoi", "coi"))) %>% 
  arrange(distribution) %>%
  rename_parameters(replacements = model_replacements) %>%
  table_cols_to_title(.) %>% 
  select(-Distribution) %>% 
  custom_kable() %>% 
  pack_rows(index = c("$\\mu$" = 24, "$\\phi$" = 24, "$\\alpha$" = 24, "$\\gamma$" = 24))
```

## Variety Exposure

```{r variety-exposure-condition-plot-testing, fig.show = "hold", out.width = "50%"}
knitr::include_graphics(c(
  plots_dir[str_detect(plots_dir, "testing_v.png")],
  plots_dir[str_detect(plots_dir, "testing_v_compare.png")]
))
```

Posterior means and credible intervals are provided in the table below.

```{r variety-exposure-condition-testing}
format_summary_table(
  model_summaries$testing_v, 
  summary_oldnames, 
  summary_newnames,
  .drop = summary_drop
  ) %>% 
  filter(.Width == 0.90) %>% 
  select(-.Width) %>% 
  custom_kable()
```

```{r variety-exposure-condition-testing-compare}
report_posteriors(
  model_summaries$testing_v_compare,
  model_summaries$testing_v_compare_rope,
  0.9,
  posterior_drop,
  posterior_oldnames,
  posterior_newnames
) %>% 
  custom_kable(., add_footer = TRUE, model_summaries$testing_v_compare_rope)
```

While the ROPE is undecided, there does not seem to be any reliable differences across the Variety Exposure conditions in regards to overall performance.

## Variety Exposure for Novel Words Only

```{r variety-exposure-novel-condition-plot-testing, fig.show = "hold", out.width = "50%"}
knitr::include_graphics(c(
  plots_dir[str_detect(plots_dir, "testing_v_n.png")],
  plots_dir[str_detect(plots_dir, "testing_v_n_compare.png")]
))
```

Posterior means and credible intervals are provided in the table below.

```{r variety-exposure-novel-condition-testing}
format_summary_table(
  model_summaries$testing_v_n, 
  summary_oldnames, 
  summary_newnames,
  .drop = summary_drop
  ) %>% 
  filter(.Width == 0.90) %>% 
  select(-.Width) %>% 
  custom_kable()
```

```{r variety-exposure-novel-condition-testing-compare}
report_posteriors(
  model_summaries$testing_v_n_compare,
  model_summaries$testing_v_n_compare_rope,
  0.9,
  posterior_drop,
  posterior_oldnames,
  posterior_newnames
) %>% 
  custom_kable(., add_footer = TRUE, model_summaries$testing_v_n_compare_rope)
```

A similar pattern appears for the novel words only as with all words.

## Word Type by Variety Exposure

We also looked at whether there are any differences in performance for different word types across conditions during the testing phase. There appears to be some differences in word types within groups, but are these differences reliable?

```{r task-variety-word-type-testing-plot, fig.show = "hold", out.width = "50%"}
knitr::include_graphics(c(
  plots_dir[str_detect(plots_dir, "testing_tvw.png")],
  plots_dir[str_detect(plots_dir, "testing_tvw_compare.png")]
))
```

Posterior means and credible intervals are provided in the table below.

```{r task-variety-word-type-testing}
format_summary_table(
  model_summaries$testing_tvw, 
  summary_oldnames, 
  summary_newnames,
  .drop = summary_drop
  ) %>% 
  filter(.Width == 0.90) %>% 
  select(-.Width) %>% 
  custom_kable()
```

We can also directly compare the differences in performance for contrastive words relative to non-contrastive words.

```{r task-variety-word-type-testing-compare}
report_posteriors(
  model_summaries$testing_tvw_compare,
  model_summaries$testing_tvw_compare_rope,
  0.9,
  posterior_drop,
  posterior_oldnames,
  posterior_newnames
) %>% 
  custom_kable(., add_footer = TRUE, model_summaries$testing_tvw_compare_rope)
```

These results reflect those in the plots above. Are any differences reported here reliable? 

It seems that there is a clear effect of word type (for contrastive vs. non-contrastive words) for the reading task in the Dialect Literacy condition. Additionally, there is some evidence for an effect of word type in the reading task for the Variety Mismatch Social and Variety Mismatch conditions, with only around 20% of the HDI within the the equivalence bounds, and *pd*s over 90%, indicating that around 90% of the posterior is of the median's sign. There is also some evidence for a word type effect in the spelling task in the Dialect Literacy condition, with only around 7% of the HDI within the equivalence bounds, and a *pd* of over 97%. This suggests that reading performance is impaired for contrastive words only when participants are exposed to a dialect, while spelling performance is only impaired when exposed to a dialect literacy intervention.

However, all other contrasts show that a large proportion of the HDI is contained by the ROPE, which indicates that we have no strong evidence for a presence or absence of an effect of Word Type.

Finally, we ask whether or not the contrastive effect is stronger in the Variety Mismatch Social condition relative to the Variety Mismatch condition.

```{r task-variety-word-type-ms-testing-compare}
report_posteriors(
  model_summaries$testing_tvw_ms_compare,
  model_summaries$testing_tvw_ms_compare_rope,
  0.9,
  posterior_drop,
  posterior_oldnames,
  posterior_newnames
) %>% 
  custom_kable(., add_footer = TRUE, model_summaries$testing_tvw_ms_compare_rope)
```

Any differences here are slight, and are mainly contained by the ROPE in both tasks. This suggests that there are no substantial differences in the magnitude of the effect between contrastive and non-contrastive words across these two conditions.

# Exploratory Covariate Testing Model

A summary of the Testing Phase model incorporating the mean scores in the vocabulary test as a covariate is provided below. This can be used to determine model diagnostics and coefficients. As with previous models, draws from the posterior for different combinations of conditions were taken using the tidybayes R-package. Similarly, hypothesis tests are provided in the form of Region of Practical Equivalence (ROPE) analyses from these draws using the bayestestR R-package. Extreme caution is needed for interpreting such hypothesis tests as the following models are purely exploratory.

```{r testing-cov-model-summary}
fitted_models[["testing_cov_model_summary"]]$fixed %>%
  as.data.frame() %>% 
  rownames_to_column(., var = "Parameter") %>% 
  merge_CI_limits(., `l-95% CI`, `u-95% CI`, decimals = 3) %>%
  select(Parameter:Est.Error, interval, everything()) %>% 
  split_dists() %>% 
  mutate(distribution = factor(distribution, levels = c("mu", "phi", "zoi", "coi"))) %>% 
  arrange(distribution) %>%
  rename_parameters(replacements = model_replacements) %>%
  table_cols_to_title(.) %>% 
  select(-Distribution) %>% 
  custom_kable() %>% 
  pack_rows(index = c("$\\mu$" = 48, "$\\phi$" = 48, "$\\alpha$" = 48, "$\\gamma$" = 48))
```

We first explored whether mean vocabulary test performance (i.e. in terms of mean nLED) predicts testing performance, and whether or not this varies across Task, Variety Exposure condition, and Word Type. A plot of this relationship is shown below.

```{r cov-variety-exposure-task-novel-plot-testing-cov}
knitr::include_graphics(
  plots_dir[str_detect(plots_dir, "testing_cov_etv.png")]
)
```

It's quite difficult to make out an overall pattern here, so instead we performed a median split based on the vocabulary test performance, and we categorised these into participants with high and low nLEDs in the vocabulary test relative to the median score.

## Word Type by Task, Variety Exposure, and Vocabulary Test Performance

First, we looked at the effect of word type across participants with high and low mean nLEDs in the vocabulary testing phase split by task and variety exposure condition.

```{r variety-exposure-condition-plot-testing-cov}
knitr::include_graphics(c(
  plots_dir[str_detect(plots_dir, "testing_cov_median_etvw.png")],
  plots_dir[str_detect(plots_dir, "testing_cov_median_etvw_compare.png")]
))
```

```{r cov-task-task-variety-word-type-ms-testing-cov}
format_summary_table(
  model_summaries$testing_cov_median_etvw, 
  summary_oldnames, 
  summary_newnames,
  .drop = summary_drop
  ) %>% 
  filter(.Width == 0.90) %>% 
  rename(`Exposure Test nLED Group` = `Exposure Test Nled Group`) %>% 
  select(-.Width) %>% 
  custom_kable()
```

There appears to be some differences across conditions here. Is this borne out in the data?

```{r cov-task-task-variety-word-type-ms-testing-cov-summary}
report_posteriors(
  model_summaries$testing_cov_median_etvw_compare,
  model_summaries$testing_cov_median_etvw_compare_rope,
  0.9,
  posterior_drop,
  posterior_oldnames,
  posterior_newnames
) %>% 
  rename(`Exposure Test nLED Group` = `Exposure Test Nled Group`) %>% 
  custom_kable(., add_footer = TRUE, model_summaries$testing_cov_median_etvw_compare_rope)
```

There is a clear word type effect in all of the dialect intervention conditions for the reading task, but only when participants have a low mean nLED score from the vocabulary test. When participants have a high nLED in the reading task, the only consistent word type effect is in the Dialect Literacy condition. In the spelling task, there is only a consistent word type effect for the Dialect Literacy condition regardless of performance in the vocabulary testing phase. Thus, performance is only worse for contrastive words in the reading task in the three dialect conditions when performance in the vocabulary test shows that the dialect form of the language is sufficiently entrenched. Poor performance in the vocabulary testing phase indicates that the dialect form of the language is not sufficiently entrenched prior to training in the standard form of the language so that no word type effects can occur. This effect is shown in both tasks and for participants with high and low nLEDs in the vocabulary testing phase for the Dialect Literacy condition because this condition interleaves the dialect form of the language with the standard form of the language during training (rather than front-loaded prior to the vocabulary test), which allows for sufficient entrenchment of the dialect form of the language to cause a great deal of local interference in both tasks.

## Variety Exposure by Task and Vocabulary Test Performance for Novel Words Only

We next focussed on novel words to see whether any differences occur for novel word decoding. The following analyses summarise the patterns in the covariate model.

```{r cov-variety-exposure-task-novel-median-plot-testing-cov}
knitr::include_graphics(c(
  plots_dir[str_detect(plots_dir, "testing_cov_median_etv_n.png")],
  plots_dir[str_detect(plots_dir, "testing_cov_median_etv_n_compare.png")]
))
```

It's clear from the figure that performance is generally worse in the testing phase for those with high mean nLEDs during the vocabulary test. Are there any differences if we directly compare these difference scores? There seems to be a pattern such that Variety Match and Mismatch differ for the spelling task, while all other contrasts seem to indicate equivalent performance across conditions. The following analysis looks at whether there are any differences between participants who had low and high nLEDs (relative to the median) in the vocabulary testing phase split by task and variety exposure condition.

```{r novel-variety-task-cov-testing-cov}
format_summary_table(
  model_summaries$testing_cov_median_etv_n, 
  summary_oldnames, 
  summary_newnames,
  .drop = summary_drop
  ) %>% 
  rename(`Exposure Test nLED Group` = `Exposure Test Nled Group`) %>% 
  filter(.Width == 0.90) %>% 
  select(-.Width) %>% 
  custom_kable()
```

We explored whether there are any differences between the variety exposure conditions in terms of how much the low and high performers vary. Is there any difference by Varity Exposure groups for their difference scores between high and low performers?

```{r novel-variety-task-cov-testing-cov-summary}
report_posteriors(
  model_summaries$testing_cov_median_etv_n_compare,
  model_summaries$testing_cov_median_etv_n_compare_rope,
  0.9,
  posterior_drop,
  posterior_oldnames,
  posterior_newnames
) %>% 
  rename(`Exposure Test nLED Group` = `Exposure Test Nled Group`) %>% 
  custom_kable(., add_footer = TRUE, model_summaries$testing_cov_median_etv_n_compare_rope)
```

Looking at differences split by task, the four variety conditions do not differ from one another in terms of differences in performance between participants with high and low mean nLEDs during the vocabulary testing phase. This suggests that there is no reliable difference in terms of novel word decoding between participants with high and low mean nLEDs in the vocabulary test across the variety exposure conditions.

**TODO**: 

- add in task and variety exposure for novel words to testing phase to the report.

- ev_n summary needed: vocab test and variety for novel words.
- etv_n summary needed: vocab test, task, and variety for novel words.

- ev_n plots (regular and contrastive) needed
- etv_n plots (regular and contrastive) needed

- change current etv_n to t_ev_n.
- change current etv_n to t_ev_n in the current document. This analysis is probably not needed.

- regular variety exposure needed for novel words? Needs summary and plots.

- work on renaming function for large tables of coefficients.
- want it to split by mu, phi, zoi, and coi (as the greek letters)

- Fix problem with "\times" not showing in tables. Probably to do with formatting of uppercase etc.

- Plots for exploratory vocab testing need to go to nLED .80.
- Fix names to be consistent in all plots and papers.
- Remove titles from plots.