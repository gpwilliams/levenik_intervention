---
title: "Results"
author: "Glenn Williams"
date: "14<sup>th</sup> August, 2019 (Last updated: `r format(Sys.time(), '%d<sup>th</sup> %B, %Y')`)"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: united
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r libraries, include = FALSE}
packages <- c(
  "tidyverse",
  "rlang",
  "here",
  "brms",
  "tidybayes",
  "bayestestR",
  "modelr",
  "ggforce",
  "ggrepel",
  "ggridges"
)

# load packages
lapply(packages, library, character.only = TRUE)

# get citations
citations <- purrr::map(packages, citation)
```

```{r functions, include = FALSE}
cite_package <- function(package){
  #' Return the name and installed version of a package as a character.
  #' @param package A character for an installed package.
  #' @return A character defining the \code{package} name with version number in brackets.
  #' @examples
  #' cite_package("tidyverse")
  #' cite_package("lme4")
  paste0(package, " [Version ", packageVersion(package), "]")
}

cite_packages <- function(package_string, as_string = TRUE){
  #' Return the name and installed version of one or more installed packages passed as a character vector.
  #' @param package A character vector of one or more installed packages.
  #' @param as_string Logical determining whether the returned output is formatted as a string in English (TRUE) or as a list (FALSE). Defaults to TRUE.
  #' @return A string or list of packages from \code{package_string} with version number in brackets.
  #' @examples
  #' cite_packages("tidyverse", as_string = TRUE)
  #' cite_packages("tidyverse", as_string = FALSE)
  #' cite_packages(c("tidyverse", "lme4"))
  if(length(package_string) > 1) {
    all_packages <- base::lapply(package_string, cite_package)
    if(as_string == TRUE) {
      but_last_packages <- toString(all_packages[1:length(all_packages)-1])
      last_package <- base::toString(all_packages[length(all_packages)])
      base::paste0(but_last_packages, ", and ", last_package)
    } else if(as_string == FALSE){
      all_packages
    }
  } else {
    if(as_string == TRUE) {
      base::toString(cite_package(package_string))
    } else if(as_string == FALSE) {
      cite_package(package_string)
    }
  }
}

rename_table_cols <- function(.data, .oldnames, .newnames) {
  names(.oldnames) <- .newnames
  
  .data %>% 
    dplyr::rename(!!!.oldnames)
}

table_cols_to_title <- function(.data) {
  .data %>% 
    dplyr::rename_all(
      ~stringr::str_to_title(base::gsub("_", " ", .))
    ) 
}

merge_CI_limits <- function(data, lower, upper, round = TRUE, decimals = 2) {
  # Pastes two columns together in brackets separated by a comma
  # Use: used to paste lower and upper CI together after tidying model with
  #       tidy_ordinal_model()
  # Inputs: data = data.frame with lower and upper CI bounds as columns
  #         lower = bare (unquoted) name of lower confidence limit column name
  #         upper = bare (unquoted) name of upper confidence limit column name
  #         round = TRUE (default), logical indicating whether or not to round
  #                 values in the CI columns prior to pasting (recommended)
  #         decimals = 2 (default), integer indicating how many values to
  #             round numbers by if rounding occurs. Ignored if round = FALSE.
  # Returns: data.frame with merged CI levels in one column, with prior
  #           lower and upper bound columns removed.
  
  # force standard evaluation; used with !! later
  
  if(round == TRUE) {
    data %>% mutate("interval" = paste0(
      "[", 
      # formatC used to force R to keep trailing zeroes
      formatC(round({{lower}}, decimals), format = "f", digits = decimals), 
      ", ", 
      formatC(round({{upper}}, decimals), format = "f", digits = decimals),
      "]"
    )) %>%
      select(-{{lower}}, -{{upper}})
  } else {
    data %>% 
      mutate("interval" = paste0("[", {{lower}}, ", ", {{upper}}, "]")) %>%
      select(-{{lower}}, -{{upper}})
  }
}

format_summary <- function(.data, .oldnames, .newnames, .lower = .lower, .upper = .upper, .drop) {
  .data %>% 
    dplyr::select(-{{.drop}}) %>%
    merge_CI_limits(., {{.lower}}, {{.upper}}, decimals = 3) %>% 
    rename_table_cols(., .oldnames, .newnames) %>%
    table_cols_to_title(.)
}

make_rope_footer <- function(.data, .lower = "ROPE_low", .upper = "ROPE_high", .CI = "CI"){
  paste0(
    "ROPE range = [", 
    .data[[.lower]][[1]],
    ", ",
    .data[[.upper]][[1]],
    "]. ROPE determined at the ",
    .data[[.CI]][[1]],
    "% CI of the HDI."
  )
}
```

```{r get-file-paths, include = FALSE}
fitted_models_dir <- list.files(
  path = here("04_analysis", "01_models"),
  pattern = "rds$",
  full.names = TRUE
)

model_summaries_dir <- list.files(
  path = here("04_analysis", "02_summaries", "02_full-data"),
  pattern = "csv$",
  full.names = TRUE
)

plots_dir <- list.files(
  path = here("03_plots", "02_full-data"),
  pattern = "png$",
  full.names = TRUE
)
```

```{r load-data, include = FALSE}
fitted_models <- purrr::map(fitted_models_dir, read_rds) %>% 
  purrr::map(., summary)

names(fitted_models) <- fitted_models_dir %>% 
  sub(".*/", "", .) %>% 
  substr(., 1, nchar(.)-4)

model_summaries <- purrr::map(model_summaries_dir, read_csv)
names(model_summaries) <- model_summaries_dir %>% 
  sub(".*/", "", .) %>% 
  substr(., 1, nchar(.)-4)
```

```{r define-global-variables}
summary_oldnames <- c(".value", ".width")
summary_newnames <- c("Mean", "Width")
summary_drop <- c(".point", ".interval") # for some reason can't be in a list.

rope_oldnames <- c("ROPE_Percentage", "ROPE_Equivalence")
rope_newnames <- c("Percentage", "Equivalence")
rope_drop <- c("CI", "ROPE_low", "ROPE_high")
```

`r version$version.string` and the R-packages `r cite_packages(packages)` were used for data preparation, analysis, and presentation.

# Models

Bayesian distributional models were fitted using the brms R-package. These models account for the fact that nLEDs are bounded between 0 and 1, with inflated counts at these bounds, which results in non-normal distributions. Crucially, in contrast to models fitted under the assumption of normality, these models do not make predictions outside the possible range of values and accurately capture the larger densities at extreme values. At the time of writing, distributional models of this nature are only available for hierarchical data using the brms R-package, which requires model fitting to be performed using a Bayesian framework. As an additional benefit, Baeysian models do not suffer from the non-convergence associated with modelling complex analyses under a Frequentist framework.

## Zero-one Inflated Beta Distributions

The models were fitted using a zero-one inflated Beta distribution, which models the data as a Beta distribution for nLEDs excluding 0 and 1, and a Bernoulli distribution for binary nLEDs of 0 and 1. Thus, predictors in the model can affect four distributional parameters: $\mu$ (mu), the mean of the nLEDs excluding 0 and 1; $\phi$ (phi), the precision (i.e. spread) of the nLEDs excluding 0 and 1; $\alpha$ (alpha; termed zoi - or zero-one inflation in brms) the probability of an nLED of 0 or 1; and $\gamma$ (gamma; termed coi - or conditional-one inflation in brms), the conditional probability of a 1 given a 0 or 1 has been observed. Larger values for these parameters are associated with (a) higher mean nLEDs in the range exluding 0 and 1, (b) tighter distributions of the nLEDs in the range excluding 0 and 1 (i.e. less variance), (c) more zero-one inflation in nLEDs, and (d) more one-inflation given zero-inflation in nLEDs. Predictors in this model can influence any and all distributional parameters in the model at once. For these models, a logit link is used for the $\mu$, $\alpha$, and $\gamma$ distributional parameters, and a log link is used for the $\phi$ distributional parameter.

## Model Fitting

### Model Specification

Three models were fitted in total: (1) assessing performance across conditions during the vocabulary test prior to literacy training; (2) assessing performance across conditions during the testing phase following literacy training; and (3) assessing performance across conditions during the testing phase following literacy training using the vocabulary test performance as a predictor. This latter model was not pre-registered, but instead serves an exploratory purpose to determine whether or not any effect of dialect exposure is mediated by initial performance. In all models, estimates population-level and group-level effects are estimated for all distributional parameters, with group-level effects correlated across all parameters.

The models were described as follows:

- **Vocabulary Test Model**: nLEDs are predicted by population-level (fixed) effects of Variety Exposure condition (with four levels: Variety Match, Mismatch, Mismatch Social, and Dialect Learning) Word Type (with two levels: Contrative and Non-contrastive), and the interaction between them, and by group-level (random) effects of random intercepts and slopes of Word Type by participants, and random intercepts and slopes of Variety Exposure by item.

- **Testing Model**: nLEDs are predicted by population-level (fixed) effects of Task (with two levels: Reading and Spelling), Variety Exposure condition, and Word Type and the interaction between them, and by group-level (random) effects of random intercepts and slopes of Task and Word Type by participant, and random intercepts and slopes of Variety Exposure by item. Crucially, the interaction between the group-level effects by participant did not include the interaction between them in order to reduce model complexity.

- **Exploratory Covariate Testing Model**: nLEDs are predicted by population-level (fixed) effects of mean nLED during the Vocabulary Test, Task, Variety Exposure condition, Word Type, and the interaction between them, and by group-level (random) effects of random intercepts and slopes of Task and Word Type by participant, and random intercepts and slopes of mean nLED during the Vocabulary Test and Variety Exposure by items. Again, the interaction between the group-level effects by participant did not include the interaction between them in order to reduce model complexity.

### Model Priors

In all models, the approach was to use weakly informative, regularising priors for fitting. Where models failed to converge, these priors were adjusted, typically placing less prior weight on extreme values.

Here, priors are described first by their expected distribution, and the parameters that define that distribution. For example, a prior of $\mathcal{N}(0, 1)$ describes a normal distribution with a mean of 0 and a standard deviation of 1. Similarly, a prior of $\mathcal{logistic}(0, 1)$ describes a logistic distribution with a mean of 0 and a standard deviation of 1. Note, by default, brms restricts priors on the *SD* to be positive.

The following priors were used for the exposure model:

- Intercept
  - $\mu$: $\mathcal{N}(0, 5)$
  - $\phi$: $\mathcal{N}(0, 3)$
  - $\alpha$: $\mathcal{logistic}(0, 1)$
  - $\gamma$: $\mathcal{logistic}(0, 1)$
- Slope
  - $\mu$: $\mathcal{N}(0, 0.5)$
  - $\phi$: $\mathcal{N}(0, 1)$
  - $\alpha$: $\mathcal{N}(0, 5)$
  - $\gamma$: $\mathcal{N}(0, 0.5)$
- *SD*
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 1)$
  - $\alpha$: $\mathcal{N}(0, 5)$
  - $\gamma$: $\mathcal{N}(0, 5)$
- *SD* by Participant Number
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 5)$
  - $\alpha$: $\mathcal{N}(0, 10)$
  - $\gamma$: $\mathcal{N}(0, 10)$
- *SD* by Item
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 5)$
  - $\alpha$: $\mathcal{N}(0, 10)$
  - $\gamma$: $\mathcal{N}(0, 10)$
- Correlation
  - $LKJ(2)$
  
Weakly informative regularising priors were used for all terms. All priors were centred on 0, with standard deviations ranging from 0.5 to 10, thus allowing for a range of values with less prior probability places on extreme responses. Largely, these priors allow the posterior to be determined primarily by the data. For the slope terms, the priors assume no effect to small effects for each parameter in either direction. Weakly informative regularising priors were also used for all standard deviation terms. Finally, an $LKJ(2)$ prior was used for the correlation between terms, which acts to down-weight perfect correlations (Vasishth et al., 2018 - CITATION). These priors are in some cases more informative than initially planned following our pre-registration (which used very weakly informative priors) to improve model fit (i.e. accounting for divergences during fitting). For example, the mu intercept and slope, and gamma slope have standard deviations half as large as planned, while the standard deviation for the phi intercept is three times as large as initially planned. Additionally, 8000 iterations were used instead of 1000 and 6 were used rather than 4 chains to improve estimates in response to warnings about bulk and tail effective sample size, totalling 48,000 samples rather than the planned 4000.

For both testing models, the following priors were used:

- Intercept
  - $\mu$: $\mathcal{N}(0, 5)$
  - $\phi$: $\mathcal{N}(0, 3)$
  - $\alpha$: $\mathcal{logistic}(0, 1)$
  - $\gamma$: $\mathcal{logistic}(0, 1)$
- Slope
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 1)$
  - $\alpha$: $\mathcal{N}(0, 5)$
  - $\gamma$: $\mathcal{N}(0, 1)$
- *SD*
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 1)$
  - $\alpha$: $\mathcal{N}(0, 5)$
  - $\gamma$: $\mathcal{N}(0, 5)$
- *SD* by Participant Number
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 5)$
  - $\alpha$: $\mathcal{N}(0, 10)$
  - $\gamma$: $\mathcal{N}(0, 10)$
- *SD* by Item
  - $\mu$: $\mathcal{N}(0, 1)$
  - $\phi$: $\mathcal{N}(0, 5)$
  - $\alpha$: $\mathcal{N}(0, 10)$
  - $\gamma$: $\mathcal{N}(0, 10)$
- Correlation
  - $LKJ(2)$
  
Due to having more observations for analyses during the testing phase, both the $\mu$ and $\gamma$ slope terms use more weakly informative priors than the exposure model. This allows the data to have a larger impact on parameter estimates while having no impact on model convergence. 

### Model Checks

Posterior predictive checks were performed for all three models, comparing the observed posterior density against samples from the fitted model. Well fitting models show concordance between observed and sampled posterior densities. Plots for each model are displayed below. Grey lines indicate samples from the posterior, while black lines indicate the observed sample density.

```{r pp-check-plots}
knitr::include_graphics(plots_dir[str_detect(plots_dir, "pp")])
```

As can be seen from the plots, the posterior predictive checks indicate a generally good model fit in all instances, such that the model largely captures the shape of the data (i.e. especially capturing the 0 and 1 inflation in the testing model), but does not capture some discrepancies in the data which do not arise from any particular process (i.e. some larger densities in the testing model between the range of 0-1).

# Vocabulary Test Model

A summary of the population-level (fixed) effects for the Vocabulary Test model is provided below. This can be used to determine model diagnostics and coefficients. To answer questions pertaining to our pre-registered hypotheses, and to generate plots for these summaries, we used draws from the posterior for different combinations of conditions using the tidybayes R-package (CITATION). Hypothesis tests are then provided in the form of Region of Practical Equivalence (ROPE) analyses from these draws using the bayestestR R-package. 

```{r exposure-model-summary}
fitted_models[["exposure_model"]]$fixed %>% 
  kableExtra::kable()
```

Posterior medians and 80% and 90% credible intervals are provided for different conditions in the plots below. Table summaries also provide posterior medians along with 90% credible intervals.

In all following plots and reported statistics, summaries are provided for only the overall nLEDs taking into account all distributional parameters when sampling from the posterior. Such estaimtes are based on the median and credible interval around the median. The median was selected to summarise these models over the mean as this method is more robust to distributions with more than one mode. Thus, we do not provide individual statistics and plots for the individual distributional terms (e.g. for zero-one inflation, or conditional-one inflation) as we did not specify any hypotheses related to these individual terms. Instead, the zero-one inflated Beta models are used purely to improve model fit and to make more accurate predictions about the overall differences in nLEDs across conditions.

## Variety Exposure

```{r variety-exposure-condition-plot-exposure, fig.show = "hold", out.width = "50%"}
knitr::include_graphics(c(
  plots_dir[str_detect(plots_dir, "exposure_v.png")],
  plots_dir[str_detect(plots_dir, "exposure_v_compare.png")]
))
```

Posterior means and credible intervals are provided in the table below.

```{r variety-exposure-condition-exposure}
format_summary(
  model_summaries$exposure_v, 
  summary_oldnames, 
  summary_newnames, 
  .drop = summary_drop
  ) %>% 
  filter(Width == 0.90) %>% 
  select(-Width) %>% 
  kableExtra::kable() %>% 
  # kableExtra::add_header_above(c(" " = 2, "Credible Interval" = 2)) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE)
```

We evaluated equivalence in the nLEDs between variety exposure conditions by determining a region of practical equivalence (ROPE) by which any effects between the reported bounds are deemed to be practically equivalent to 0. In all instances, this is determined at the 90% credible interval (CI) bound of the highest density interval (HDI), which is typically more stable than larger bounds (CITATION: Kruschke). We report the proportion of the HDI contained within the ROPE region along with bounds of this interval. Where HDIs are entirely contained by the equivalence bounds, equivalence is accepted. Where HDIs are entirely outside the equivalence bounds, equivalence is rejected. Uncertaintainty is assigned to any HDIs that cross the equialence bounds in either (or both) directions.

```{r variety-exposure-condition-ROPE-exposure}
format_summary(
  model_summaries$exposure_v_compare, 
  .oldnames = rope_oldnames, 
  .newnames = rope_newnames,
  .lower = HDI_low,
  .upper = HDI_high,
  .drop = rope_drop
  ) %>% 
  mutate(Percentage = Percentage * 100) %>% 
  rename(Comparison = `Variety Exposure`) %>% 
  kableExtra::kable() %>% 
  kableExtra::add_header_above(c(" " = 1, "ROPE Coverage" = 3)) %>% 
  kableExtra::footnote(
    general = make_rope_footer(model_summaries$exposure_v_compare)
  ) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE)
```

While nLEDs in the Variety Mismatch condition are generally lower than those in the Variety Match condition, 30% of the difference scores are contained by the equivalence bounds. Similarly, while nLEDs are generally higher in the two intervention conditions (Variety Mismatch Social and Dialect Literacy) when compared to the Variety Mismatch condition, around half of the difference scores are contained by the equivalence bounds. All other differences are largely undecided.

## Word Type by Variety Exposure

We also looked at whether there are any differences in performance for different word types across conditions during the vocabulary testing phase. 

```{r variety-word-type-exposure-plot, fig.show = "hold", out.width = "50%"}
knitr::include_graphics(c(
  plots_dir[str_detect(plots_dir, "exposure_vw.png")],
  plots_dir[str_detect(plots_dir, "exposure_vw_compare.png")]
))
```

Posterior means and credible intervals are provided in the table below.

```{r variety-word-type-exposure}
format_summary(
  model_summaries$exposure_vw, 
  summary_oldnames, 
  summary_newnames, 
  .drop = summary_drop
  ) %>% 
  filter(Width == 0.90) %>% 
  select(-Width) %>%
  kableExtra::kable() %>% 
  # kableExtra::add_header_above(c(" " = 3, "Credible Interval" = 2)) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE)
```


```{r variety-word-type-exposure-ROPE}
format_summary(
  model_summaries$exposure_vw_compare, 
  .oldnames = rope_oldnames, 
  .newnames = rope_newnames,
  .lower = HDI_low,
  .upper = HDI_high,
  .drop = rope_drop
  ) %>% 
  mutate(Percentage = Percentage * 100) %>% 
  rename(Group = `Variety Exposure`, Comparison = `Word Type`) %>% 
  kableExtra::kable() %>% 
  kableExtra::add_header_above(c(" " = 2, "ROPE Coverage" = 3)) %>% 
  kableExtra::footnote(
    general = make_rope_footer(model_summaries$exposure_vw_compare)
  ) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE)
```

In all instances there is some evidence that performance is better for non-contrastive words relative to contrastive words. However, equivalence between the two word types is only confidently rejected for the Dialect Literacy condition. Thus, it is likely that performance was worse for contrastive words relative to non-contrastive words in the Dialect Literacy condition in the Vocabulary Test.





# Testing Phase Model

A summary of the Testing Phase model is provided below. This can be used to determine model diagnostics and coefficients. As with the Vocabulary Test Model, to answer questions pertaining to our pre-registered hypotheses, and to generate plots for these summaries, we used draws from the posterior for different combinations of conditions using the tidybayes R-package (CITATION). Similarly, hypothesis tests are provided in the form of Region of Practical Equivalence (ROPE) analyses from these draws using the bayestestR R-package. 

```{r testing-model-summary}
fitted_models[["testing_model"]]$fixed %>% 
  kableExtra::kable()
```

## Variety Exposure

```{r variety-exposure-condition-plot-testing}
knitr::include_graphics(
  plots_dir[str_detect(plots_dir, "testing_v_compare.png")]
)
```

Posterior means and credible intervals are provided in the table below.

```{r variety-exposure-condition-testing-ROPE}
format_summary(
  model_summaries$testing_v_compare, 
  .oldnames = rope_oldnames, 
  .newnames = rope_newnames,
  .lower = HDI_low,
  .upper = HDI_high,
  .drop = rope_drop
  ) %>% 
  mutate(Percentage = Percentage * 100) %>%
  rename(Comparison = `Variety Exposure`) %>% 
  kableExtra::kable() %>% 
  kableExtra::add_header_above(c(" " = 1, "ROPE Coverage" = 3)) %>% 
  kableExtra::footnote(
    general = make_rope_footer(model_summaries$testing_v_compare)
  ) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE)
```

While the ROPE is undecided, there does not seem to be any reliable differences across the Variety Exposure conditions in regards to overall performance.

## Variety Exposure for Novel Words Only

```{r variety-exposure-novel-condition-plot-testing}
knitr::include_graphics(plots_dir[str_detect(plots_dir, "testing_v_n_compare.png")])
```

Posterior means and credible intervals are provided in the table below.

```{r variety-exposure-novel-condition-testing-ROPE}
format_summary(
  model_summaries$testing_v_n_compare, 
  .oldnames = rope_oldnames, 
  .newnames = rope_newnames,
  .lower = HDI_low,
  .upper = HDI_high,
  .drop = rope_drop
  ) %>% 
  mutate(Percentage = Percentage * 100) %>%
  rename(Comparison = `Variety Exposure`) %>% 
  kableExtra::kable() %>% 
  kableExtra::add_header_above(c(" " = 1, "ROPE Coverage" = 3)) %>% 
  kableExtra::footnote(
    general = make_rope_footer(model_summaries$testing_v_n_compare)
  ) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE)
```

A similar pattern appears for the novel words only as with all words.

## Word Type by Variety Exposure

We also looked at whether there are any differences in performance for different word types across conditions during the testing phase. There appears to be some differences in word types within groups, but are these differences reliable?

```{r task-variety-word-type-testing-plot, fig.show = "hold", out.width = "50%"}
knitr::include_graphics(c(
  plots_dir[str_detect(plots_dir, "testing_tvw.png")],
  plots_dir[str_detect(plots_dir, "testing_tvw_compare.png")]
))
```

It seems that there is a clear effect of word type (for contrastive vs. non-contrastive words) for both tasks in the Dialect Literacy condition, and for reading only in the Variety Mismatch Social condition. Additionally, there is some evidence for an effect of word type in the Variety Mismatch condition, with only around 6% of the HDI in the the equivalence bounds. This suggests that performance is impaired for contrastive words only when participants are exposed to a dialect.

Posterior means and credible intervals are provided in the table below.

```{r task-variety-word-type-compare}
format_summary(
  model_summaries$testing_tvw, 
  summary_oldnames, 
  summary_newnames, 
  .drop = summary_drop
  ) %>% 
  filter(Width == 0.90) %>% 
  select(-Width) %>%
  kableExtra::kable() %>% 
  # kableExtra::add_header_above(c(" " = 4, "Credible Interval" = 2)) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE)
```

We can also directly compare the differences in performance for contrastive words relative to non-contrastive words.

```{r task-variety-word-type-testing-compare}
format_summary(
  model_summaries$testing_tvw_compare, 
  summary_oldnames, 
  summary_newnames, 
  .drop = summary_drop
  ) %>%
  filter(Width == 0.90) %>% 
  select(-Width) %>%
  kableExtra::kable() %>% 
  # kableExtra::add_header_above(c(" " = 4, "Credible Interval" = 2)) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE)
```

These results reflect those in the plots above. Are any differences reported here reliable?

```{r task-variety-word-type-testing-ROPE}
format_summary(
  model_summaries$testing_tvw_compare_rope, 
  .oldnames = rope_oldnames, 
  .newnames = rope_newnames,
  .lower = HDI_low,
  .upper = HDI_high,
  .drop = rope_drop
  ) %>% 
  mutate(Percentage = Percentage * 100) %>% 
  rename(Comparison = `Word Type`) %>% 
  kableExtra::kable() %>% 
  kableExtra::add_header_above(c(" " = 3, "ROPE Coverage" = 3)) %>% 
  kableExtra::footnote(
    general = make_rope_footer(model_summaries$testing_tvw_compare_rope)
  ) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE)
```

Equivalence is confidently rejected for the reading task for the two intervention conditions (Variety Mismatch Social and Dialect Literacy). Additionally, for the reading task the majority of the HDI for the Variety Mismatch condition is outside of the ROPE. For the spelling task, equivalence is rejected for the Dialect Literacy condition. However, all other contrasts show that most of the HDI is contained by the ROPE (suggesting equivalence).

Finally, we ask whether or not the contrastive effect is stronger in the Variety Mismatch Social condition relative to the Variety Mismatch condition.

```{r task-variety-word-type-ms-testing-ROPE}
format_summary(
  model_summaries$testing_tvw_ms_compare, 
  .oldnames = rope_oldnames, 
  .newnames = rope_newnames,
  .lower = HDI_low,
  .upper = HDI_high,
  .drop = rope_drop
  ) %>% 
  mutate(Percentage = Percentage * 100) %>% 
  kableExtra::kable() %>% 
  kableExtra::add_header_above(c(" " = 3, "ROPE Coverage" = 3)) %>% 
  kableExtra::footnote(
    general = make_rope_footer(model_summaries$testing_tvw_ms_compare)
  ) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE)
```

Any differences here are slight, and are mainly contained by the ROPE in both tasks. This suggests that there are no substantial differences in the magnitude of the effect between contrastive and non-contrastive words across these two conditions.