---
title: "Results"
author: "Glenn Williams"
date: "14<sup>th</sup> August, 2019 (Last updated: `r format(Sys.time(), '%d<sup>th</sup> %B, %Y')`)"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: united
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)

packages <- c(
  "tidyverse",
  "rlang",
  "here",
  "brms",
  "tidybayes",
  "bayestestR",
  "modelr",
  "ggforce",
  "ggrepel",
  "ggridges",
  "irr"
)

# load packages
lapply(packages, library, character.only = TRUE)

# load functions
r_function_list <- list.files(
  path = here("R", "00_functions"), 
  pattern = "R$",
  full.names = TRUE
)
purrr::walk(r_function_list, source)

# get citations
citations <- purrr::map(packages, citation)

# get file paths ----

# get model summaries only for easy loading
fitted_models_dir <- list.files(
  path = here("04_analysis", "01_models"),
  pattern = "rds$",
  full.names = TRUE
) %>% 
  str_subset("summary")
  

model_summaries_dir <- list.files(
  path = here("04_analysis", "02_summaries", "02_full-data"),
  pattern = "csv$",
  full.names = TRUE
)

plots_dir <- list.files(
  path = here("03_plots", "02_full-data"),
  pattern = "png$",
  full.names = TRUE
)

# load data ----

fitted_models <- purrr::map(fitted_models_dir, read_rds)

names(fitted_models) <- fitted_models_dir %>% 
  sub(".*/", "", .) %>% 
  substr(., 1, nchar(.)-4)

model_summaries <- purrr::map(model_summaries_dir, read_csv)
names(model_summaries) <- model_summaries_dir %>% 
  sub(".*/", "", .) %>% 
  substr(., 1, nchar(.)-4)
```

```{r define-renaming-variables}
summary_oldnames <- c(".value", "interval")
summary_newnames <- c("median", "percentile interval")
summary_drop <- c(".point", ".interval")

posterior_oldnames <- c("Ci Interval", "Rope Interval", "Pd")
posterior_newnames <- c("Percentile Interval", "HDI Interval", "P(Direction)")
posterior_drop <- c(
  ".width",
  ".point",
  ".interval",
  "CI",
  "ROPE_low", 
  "ROPE_high",
  "ROPE_Equivalence"
)
```

`r version$version.string` and the R-packages `r cite_packages(packages)` were used for data preparation, analysis, and presentation.

# Exploratory Covariate Testing Model

A summary of the Testing Phase model incorporating the mean scores in the vocabulary test as a covariate is provided below. This can be used to determine model diagnostics and coefficients. As with previous models, draws from the posterior for different combinations of conditions were taken using the tidybayes R-package (CITATION). Similarly, hypothesis tests are provided in the form of Region of Practical Equivalence (ROPE) analyses from these draws using the bayestestR R-package. Extreme caution is needed for interpreting such hypothesis tests as the following is purely exploratory.

```{r testing-cov-model-summary, include = FALSE}
summary(fitted_models[["testing_cov_model"]]$fixed)
```

We first explored whether mean vocabulary test performance (in nLED) predicts testing performance, and whether or not this varies across Task, Variety Exposure condition, and Word Type. A plot of this relationship is shown below.

```{r cov-variety-exposure-task-novel-plot-testing-cov, include = FALSE}
knitr::include_graphics(plots_dir[str_detect(plots_dir, "testing_cov_etv")])
```

## Variety Exposure

It's quite difficult to make out an overall pattern here, so instead we performed a median split based on the vocabulary test performance, and we categorised these into those with high and low nLEDs in the vocabulary test.

# Word Type by Task, Variety Exposure, and Exposure Test Performance

```{r}
model_summaries$testing_cov_median_etvw
model_summaries$testing_cov_median_etvw_compare
```





## Variety Exposure for Novel Words Only

We next focussed on novel words to see whether any differences occur for novel word decoding. The following analyses summarise the patterns in the covariate model. 

```{r cov-variety-exposure-task-novel-median-plot-testing-cov, include = FALSE}
knitr::include_graphics(plots_dir[str_detect(plots_dir, "testing_cov_median_etv_n")])
```

It's clear from the figure that performance is generally worse in the testing phase for those with high mean nLEDs during the vocabulary test. Are there any differences if we directly compare these difference scores?

```{r cov-variety-exposure-task-novel-median-compare-plot-testing-cov, include = FALSE}
knitr::include_graphics(plots_dir[str_detect(plots_dir, "testing_cov_median_etv_n_compare")])
```

Variety Match and Mismatch seem to differ for the spelling task, while all other contrasts seem to indicate equivalent performance across conditions. Is this borne out in the data?



How do high and low performers compare by task and variety exposure condition?

Is there any difference by Varity Exposure groups for their difference scores between high and low performers?

```{r}
model_summaries$testing_cov_median_etv_n
model_summaries$testing_cov_median_etv_n_compare
```









```{r novel-variety-task-cov-testing-cov-summary-ROPE, include = FALSE}
format_summary(
  model_summaries$testing_cov_median_etv_n_compare, 
  .oldnames = rope_oldnames, 
  .newnames = rope_newnames,
  .lower = HDI_low,
  .upper = HDI_high,
  .drop = rope_drop
  ) %>% 
  rename(`Vocab Test nLED Group` = `Exposure Test Nled Group`) %>% 
  kableExtra::kable() %>% 
  kableExtra::add_header_above(c(" " = 3, "ROPE Coverage" = 3)) %>% 
  kableExtra::footnote(
    general = make_rope_footer(model_summaries$testing_cov_median_etv_n_compare)
  ) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE)
```

Here, there is evidence that spelling performance between those with high and low nLEDs in the vocabulary test varies between the Variety Match and Variety Mismatch conditions. Viewing the first figure, it is clear that there is a larger discrepancy in performance between those with high and low nLEDs in the vocabulary test in the Variety Mismatch condition relative to the Variety Match condition. While there is a similar trend in the other two variety conditions (i.e. Variety Mismatch Social and Dialect Literacy), there is not enough evidence here to support the claim that the differences between those with high and low nLEDs in the vocabulary test vary substantially in the Variety Mismatch Social and Dialect Literacy conditions relative to the Variety Mismatch condition.

Next, we looked at whether or not contrastive words differed to non-contrastive words across the high and low nLED groups. First off, what do the nLEDs look like in these groups?

```{r variety-exposure-condition-plot-testing-cov, include = FALSE}
knitr::include_graphics(plots_dir[str_detect(plots_dir, "testing_cov_etvw_median")])
```

That's quite a lot to parse in one go, so we can instead look at the difference scores between contrastive and non-contrastive words for this same comparison.

```{r variety-exposure-condition-plot-compare-testing-cov, include = FALSE}
knitr::include_graphics(plots_dir[str_detect(plots_dir, "testing_cov_median_etvw_compare")])
```

In the Variety Mismatch condition, the difference in the reading task between the two word types seems to stem from those with low nLEDs during the vocabulary test, while the difference is consistent across groups in both of the Dialect Intervention conditions. But, any effect in spelling is only found for those with high nLEDs in the vocabulary test following the Dialect Literacy training. Does the data back this up?

```{r word-type-variety-exposure-task-cov-testing-cov-summary-ROPE, include = FALSE}
format_summary(
  model_summaries$testing_cov_median_etvw_compare, 
  .oldnames = rope_oldnames, 
  .newnames = rope_newnames,
  .lower = HDI_low,
  .upper = HDI_high,
  .drop = rope_drop
  ) %>% 
  dplyr::rename(`Vocab Test nLED Group` = `Exposure Test Nled Group`) %>% 
  dplyr::select(
    `Variety Exposure`, 
    Task, 
    `Vocab Test nLED Group`, 
    everything()
  ) %>% 
  kableExtra::kable() %>% 
  kableExtra::add_header_above(c(" " = 4, "ROPE Coverage" = 3)) %>% 
  kableExtra::footnote(
    general = make_rope_footer(model_summaries$testing_cov_median_etvw_compare)
  ) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE)
```

**NOTE**: The posterior samples for the covariate models is relatively low when using the median split. Increasing the number of samples may give us more stable estimates.