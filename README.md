# Williams, G. P., Panayotov, N., & Kempe, V. (2020, July 23). Exposure to dialect variation in an artificial language prior to literacy training impairs reading of words with competing variants but does not affect decoding skills.<sup id="1">[1](#f1)</sup>

This repository contains the materials, data, code, and analysis product necessary to reproduce the analyses described in Williams, Panayotov, & Kempe (2020). [*Exposure to dialect variation in an artificial language prior to literacy training impairs reading of words with competing variants but does not affect decoding skills.*](https://osf.io/7ct9x/).

This repository, including all pre-registrations and deviations from registrations, is hosted on the [OSF](https://osf.io/7ct9x/). The [source code for the experiments](https://github.com/gpwilliams/language-learning-experiment) is hosted on GitHub.

Please see the metadata file for a description of the files and folders in this repository.

This work is licensed under [CC-By Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/legalcode).

Note that Bayesian models are saved in full only via the [OSF repository](https://osf.io/5mtdj/). Please check the metadata file for further information on these models.

<b id="f1">1</b> Why Levenik? In the lab we called our made-up language Levenik for brevity, referring to the names of the language creators; G**le**nn, **Ve**ra, and **Nik**olay. [â†©](#a1)



